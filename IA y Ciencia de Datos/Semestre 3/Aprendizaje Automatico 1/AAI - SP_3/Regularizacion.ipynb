{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Lineales con Regularizaci칩n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:07:46.932981Z",
     "start_time": "2020-09-03T21:07:46.929505Z"
    }
   },
   "source": [
    "### El problema del Overifiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los problemas que solemos enfrentar es el **overfiting**, el cual se produce cuando nuestro modelo es \"demasiado complejo\" y comienza a aprender de casos particulares del Train Set y  del ruido que tenemos en nuestras observaciones, pareci칠ndose *demasiado* a las particularidades de los datos del Train Set y por lo tanto teniendo una baja performance en la generalizaci칩n al medir el Error en el Test Set.   \n",
    "\n",
    "Resumiendo, **nuestro modelo tiene overfiting cuando anda muy bien en el Train Set y anda mal al pronosticar en el Test Set**.\n",
    "\n",
    "> Los m칠todos de Regularizaci칩n Ridge, Lasso y Elastic - Net nos permitir치n crear modelos con menor **overfiting**. Tipicamente se aplican para los modelos de Regresi칩n Lineal y Regresi칩n Log칤stica ya que se basan en modificar los valores que se obtendr칤an de los par치metros de estos modelos.\n",
    "\n",
    "Pensemos en un caso conocido por nosotros como es el de Regresi칩n Lineal, con una variable $x$ predictora y una variable $y$ que deseamos pronosticar. En ese caso nuestra hip칩tesis pod칤a tomar la forma m치s sencilla:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x $$  \n",
    "\n",
    "La cual pod칤치 ser 칰til si el gr치fico de y(x) ten칤a aspecto de una recta. Pero si su aspecto era distinto entonces prob치bamos con polinomios de distinto grado:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x + w_2 x^2 + ... + w_n x^n$$  \n",
    "\n",
    "Pero al aumentar el grado de polinomio, nuestra h(x) comienza a adquirir la posibilidad de subir y bajar con un mayor *grado de libertad* y por lo tanto  a \" adapatarse\" demasiado bien a los puntos individuales del Train Set, de tal manera que comenzaba a de casos particulares dentro del Train y tambi칠n a \"aprender\" del ruido y puede generalizar mal.   \n",
    "\n",
    "El exceso de par치metros $w_i$ de nuestra hip칩tesis es lo que brinda el **grado de libertad** de nuestro modelo.  \n",
    "\n",
    "Es decir que: \n",
    "\n",
    "> Si tenemos *demasidos* par치metros $w_i$ nuestra hip칩tesis pasa a tener demasiados *grados de libertad* y aumenta la tendencia al overfitting.  \n",
    "\n",
    "Ahora bien, 칠sto no ocurre solamente cuando h(x) corresponde a un polinomio de potencias de x, tambi칠n ocurrir치 cuando tengamos **muchas features**, particularmente si algunas de ellas no son muy explicativas del valor de y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que contamos en nuestro dataset con n features = $ {x_1,x_2, ...,x_n}$ y la variable num칠rica $ y $ que deseamos pronosticar. Nuestra hip칩tesis podr칤a ser de la forma:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n $$  \n",
    "\n",
    "Si la cantidad de variables $x_i$ es excesiva nuestro modelo podr치 desarrollar overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Una de las maneras de intentar disminuir el overfitting es utilizar menos features, la pregunta ser칤a entonces, cu치les no usar.   \n",
    "\n",
    "- Existen m칠todos para ver cu치les de las features nos brindan poca o nula informaci칩n con respecto a la variable $y$ que queremos pronosticar (y de esta manera eliminarlas de nuestro dataset). Pero tambi칠n existe otro m칠todo que nos permite reducir el overfitting, la **Regularizaci칩n**, y quiz치 no necesitemos quitar demasiadas features que tangan valor explicativo para nuestro modelo.  \n",
    "\n",
    "La idea central para el concepto de regularizaci칩n consiste en ampliar un poco la idea de los *grados de libertad*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pogamos por caso una hip칩tesis como la polin칩mica, porque es m치s f치cil de pensar, pero lo mismo servir칤a cuando las features no son potencias de una variable:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x + w_2 x^2 + ... + w_n x^n$$    \n",
    "\n",
    "Dijimos que si n era demasiado grande, un polinomio de grado alto puede adaptarse pr치cticamente a cualquier conjunto de puntos que querramos, lo cual generalizamos ahora diciendo que tiene un alto grado de libertad.  \n",
    "\n",
    "Ahora bien qu칠 pasar칤a si imponemos la condici칩n de que $w_n = 0$?  Indudablemente el polinomio ahora tendr칤a menor potencia y por ende menor grado de libertad.   Uno puede pensar que es el mismo efecto que haber quitado directamente a $x^n$, lo cual es cierto porque le asignamos un valor de 0 ... pero tambi칠n le quitar칤amos libertad si le exigimos que tome cualquier otro valor, como por ejemplo $ w_n = 4$!  \n",
    "\n",
    "Al restringir el valor de alguno de los par치metros, le quitamos grados de libertad a la expresi칩n polin칩mica!  \n",
    "\n",
    "Es cierto que restringir el valor de alguno de los par치metros $w_i$ es un caso un poco extremo ... pero podr칤amos hacer algo m치s sutil ... si el problema es porque el polinomio tiene \"potencias demasiado altas\", en vez de quitarlas directamente asign치ndoles $w_n=0$ podemos hacer algo que no la haga desaparecer, pero que *aten칰e* su influencia frente a las dem치s ... por ejemplo podemos imponer la condici칩n de que:  \n",
    "\n",
    "$$w_i < 0.1$$  \n",
    "\n",
    "de esta manera no hacemos desaparecer la potencia n del polinomio, pero es como si lo transform치ramos en uno m치s restringido, casi como pensar que es un polinomio con un grado *algo* menor pero que no llega a ser una potencia menor!  \n",
    "\n",
    "En el caso espec칤fico de la expansi칩n polin칩mica seguramente pensar칤amos en reducir el valor de los $w_i$ para las potencias m치s altas, ahora bien, la idea es totalmente aplicable al caso m치s general cuando nuestra hip칩tesis tienen muchos t칠rminos porque contamos con muchas features sin relaci칩n entre ellas:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n $$    \n",
    "\n",
    "Excepto que en este caso, para quitar grados de libertad sin quitar variables le imponemos la condici칩n de que **todos los valores de $w_i$ deban ser peque침os**.   \n",
    "\n",
    "> La idea central en la Regularizaci칩n de modelos lineales (como Regresi칩n Lineal e inclusive Regresi칩n Log칤stica) pasa por **disminuir los grados de libertad de nuestra hip칩tesis, la cual conseguiremos al restringir los valores que pueden tomar los par치metros $w_i$**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluaci칩n de los modelos con regularizaci칩n:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque efectuaremos cambios en algunas partes del modelo de Regresi칩n Lineal, \n",
    "\n",
    "> las m칠tricas de evaluaci칩n **no cambian**, seguri치n siendo las mismas de siempre: MSE, RMSE, R<sup>2</sup>, etc\n",
    "\n",
    "usadas habitualmente para los problemas de Regresi칩n y lo mismo suceder치 para los modelos de Clasificaci칩n con Regularizaci칩n.\n",
    "\n",
    "**Las m칠tricas de evaluaci칩n no dependen del modelo elegido, a칰n para los pr칩ximos modelos de ML que veremos a lo largo de la carrera.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:30:19.808369Z",
     "start_time": "2020-09-01T18:30:19.802386Z"
    }
   },
   "source": [
    "## Ridge (tambi칠n se lo conoce como Tikhonov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo Ridge restringe (a veces se dice  **penaliza**) los valores de los par치metros $w_i$ agregando un t칠rmino extra en la funci칩n de Costo J de la Regresi칩n Lineal habitual.  \n",
    "\n",
    "La hip칩tesis para Regresi칩n Lineal con $n$ features o variables es:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n $$  \n",
    "\n",
    "  \n",
    "\n",
    "Suponiendo que tengamos $m$ observaciones en nuestro Train Set, la funci칩n de Costo J habitual para Regresi칩n Lineal es la que utiliza m칤nimos cuadrados u OLS (Ordinary Least Squares), aclaremos que se podr칤a usar cualquier otra, pero dado que es la m치s usada desarrollem칩sla:\n",
    "\n",
    "$$ J = \\frac{1}{2m}  \\displaystyle\\sum_{i=1}^{m} [ h(x_{i}) - y_{i}]  $$     \n",
    "\n",
    "dado que en m es la cantidad de observaciones, lo cual es una constante, algunas implementaciones la quitan de la f칩rmula de J, ya que el resultado ser치 el mismo. Nosotros la quitaremos para que queda m치s sencilla la expresi칩n:\n",
    "\n",
    "\n",
    "$$ J = \\frac{1}{2}  \\displaystyle\\sum_{i=1}^{m} [ h(x_{i}) - y_{i}]  $$     \n",
    "\n",
    "\n",
    "\n",
    "o lo que es lo mismo, poniendo de manifiesto que J es una funci칩n de los w:\n",
    "\n",
    "$$ J = \\frac{1}{2}  \\displaystyle\\sum_{i=1}^{m} [w_0 + w_1 x_{1i} + w_2 x_{2i} + ... + w_n x_{ni}  - y_{i}]  $$\n",
    "\n",
    "Recordemos que debemos **minimizar** J con respecto a los par치metros $w$ para as칤 determinar la hip칩tesis h(x) que mejor aproxima a las m observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En Ridge agregamos un t칠rmino m치s a la funci칩n de Costo, el **T칠rmino de Regularizaci칩n de Ridge**, para restringir los valores que pueden tomar los par치metros $w_j$ y de esta manera **disminuir los grados del libertad** que tiene el modelo y de esta manera evitar el **overfitting**:\n",
    "\n",
    "$$  \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 $$    \n",
    "\n",
    "Como se basa en la suma de los cuadrados (potencia 2) de los par치metros, se suele decir que es una **penalizaci칩n $l_2$**.  \n",
    "\n",
    "$\\lambda$ es un nuevo hiperpar치metro y asume s칩lo valores positivos.\n",
    "\n",
    "Al sumarlo a la funci칩n de Costo J, la nueva funci칩n de Costo queda de la siguiente forma:  \n",
    "\n",
    "$$ J_R = J+ \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2$$   \n",
    "\n",
    "es decir:\n",
    "\n",
    "\n",
    "$$ J_R = \\frac{1}{2} ( \\displaystyle\\sum_{i=1}^{m} [ h(x_{i}) - y_{i}]  + \\lambda \\displaystyle\\sum_{j=1}^{n}  w_j^2)$$  \n",
    "\n",
    "Pasando a ser esta nueva funci칩n de Costo $J_R$ la que deberemos minimizar con respecto a los par치metros $w_j$.  \n",
    "\n",
    "Nota: El proceso de minimizaci칩n puede ser Gradient Descent y sus derivados pero tambi칠n existen formas matriciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un poco el nuevo **T칠rmino de Regularizaci칩n de Ridge**:  \n",
    "    \n",
    "$$  \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 $$    \n",
    "\n",
    "donde $  \\lambda > 0$  es un hiperpar치metro para la regularizaci칩n.\n",
    "\n",
    "En definitiva es: \n",
    "\n",
    "$$\\frac{\\lambda}{2} (w_1^2 + w_2^2 + w_3^2 ... + w_n^2)$$  \n",
    "\n",
    "Observemos que por ser una suma de n칰meros al cuadrado, su valor siempre ser치 positivo o a lo sumo 0. \n",
    "\n",
    "$$\\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 \\geq 0$$\n",
    "\n",
    "\n",
    "\n",
    "$n$ es la cantidad de features o variables de nuestro modelo lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos que:  \n",
    "    \n",
    "- $w_0$ **no**  forma parte del T칠rmino de Ridge, es decir no restringimos el valor que puede tomar.  \n",
    "\n",
    "- Al agregar este t칠rmino, como estar치n sujetos a la minimizaci칩n de $J_R$, los $w_1,w_2 ..., w_n$ no podr치n tomar cualquier valor, sino que deber치n mantenerse peque침os para que $J_R$ sea minimizada.\n",
    "\n",
    "- $\\lambda$ es un valor positivo,  es un **hiperpar치metro** de Regularizaci칩n y sirve para determinar \"cu치nta regularizaci칩n\" queremos que tenga el modelo:  \n",
    "\n",
    "    - Si $ \\lambda$ es = 0, $J_R$ ser칤a igual que la $J$ original, as칤 que el modelo **no tendr칤a Regularizaci칩n**\n",
    "    - Si $ \\lambda$ es un valor *grande*, entonces como todo el t칠rmino de Regularizaci칩n:\n",
    "    \n",
    "    $$\\frac{\\lambda}{2} (w_1^2 + w_2^2 + w_3^2 ... + w_n^2)$$\n",
    "    \n",
    "    tiene que ser peque침o, deber치 ocurrir que   $$(w_1^2 + w_2^2 + w_3^2 ... + w_n^2)$$   \n",
    "    sea *muy peque침o*, lo cual significa que le estaremos exigiendo una **gran regularizaci칩n**.  \n",
    "    \n",
    "En definitiva:   \n",
    "\n",
    "- si elegimos un valor de $\\lambda$ peque침o el modelo tendr치 poca regularizaci칩n\n",
    "- si elegimos un valor de  $\\lambda$ grande el modelo tendr치 mucha regularizaci칩n\n",
    "\n",
    "Al modelizar, deberemos ajustar el valor de $\\lambda$ probando con varios de ellos para determinar con qu칠 nivel de regularizaci칩n obtenemos mejores resultados, por ejemplo con el valor del RMSE.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T18:58:13.354910Z",
     "start_time": "2020-09-03T18:58:13.351920Z"
    }
   },
   "source": [
    "### Ridge en Scikit - Learn\n",
    "\n",
    "Antes de aplicar, es preferente estandarizar los datos.\n",
    "\n",
    "Recordemos que en Scikit-Learn la sintaxis es muy coherente para cualquier modelo que deseemos utilizar, en esencia:  \n",
    "\n",
    "- instanciar el modelo\n",
    "- .fit\n",
    "- .predict\n",
    "- .score\n",
    "\n",
    "En sklearn el **hiperpar치metro $\\lambda$ se denomina alpha**\n",
    "\n",
    "En el caso de Ridge, la librer칤a que debemos importar es: sklearn.linear_model.Ridge, la documentaci칩n oficial se encuentra aqu칤:  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "\n",
    "El uso b치sico se ver칤a:\n",
    "\n",
    "~~~\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "modelo_ridge=Ridge(alpha=0.25)\n",
    "modelo_ridge.fit(X_train,y_train)\n",
    "modelo_ridge.score(X_test,y_test)\n",
    "modelo_ridge.predict(X_nuevos)\n",
    "~~~\n",
    "\n",
    "Al crear el modelo en la primer l칤nea hay una opci칩n muy interesante que permite elegir el m칠todo paa minimizar $J_R$ , por defecto la opci칩n es \"auto\" y con ella sklearn elige la que considera mejor forma seg칰n el tipo de datos. Po rlo general las opciones por defecto funcionan bien, y uno puede dedicarse a probar valores de alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T17:53:14.347475Z",
     "start_time": "2020-09-03T17:53:14.344004Z"
    }
   },
   "source": [
    "## Lasso (Least Absolute Shrinkage and Selection Operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de Regularizaci칩n Lasso es muy parecido a Ridge, tambi칠n se adiciona un t칠rmino siempre positivo a la funci칩n de Costo J, pero esta vez la **penalizaci칩n es $l_1$**, es decir que en vez de sumar los cuadrados de lo $w_j$, sumaremos simplemente sus valores absolutos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El T칠rmino de Regularizaci칩n Lasso es entonces:  \n",
    "    \n",
    "$$  \\lambda \\displaystyle\\sum_{j=1}^{n}  | w_j | $$    \n",
    "\n",
    "Al sumarlo a la funci칩n de Costo J, la nueva funci칩n de Costo queda de la siguiente forma:  \n",
    "\n",
    "$$ J_L = J+ \\lambda \\displaystyle\\sum_{j=1}^{n}  |w_j|$$   \n",
    "\n",
    "es decir:\n",
    "\n",
    "\n",
    "$$ J_L = \\frac{1}{2}  \\displaystyle\\sum_{i=1}^{m} [ h(x_{i}) - y_{i}]  + \\lambda \\displaystyle\\sum_{j=1}^{n}  |w_j|$$  \n",
    "\n",
    "Pasando a ser esta nueva funci칩n de Costo $J_L$ la que deberemos minimizar con respecto a los par치metros $w_j$.  \n",
    "\n",
    "\n",
    "Todas las consideraciones que hemos hecho anteriormente para Ridge son tambi칠n v치lidas para Lasso, es decir que un valor de $\\lambda$ peque침o equivale a decir que estamos aplicando poca regularizaci칩n y los valores de los par치metros ser치n muy similares a los que obtendr칤amos en una regresi칩n lineal sin regularizaci칩n y un valor de $\\lambda$ grande, una fuerte regularizaci칩n, lo cual indicar칤a que tenderiamos a disminuir fuertemente los valores de los par치metros.  \n",
    "\n",
    "Por supuesto, habr치 que probar con distintos valores de este hiperp치rametro hasta encontrar su valor 칩ptimo que ser치 con el que obtengamos por ejemplo el menor RMSE al evaluar el modelo..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecci칩n de features o variables con Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso trae un **bonus** de regalo. Veamos\n",
    "\n",
    "Como hemos planteado anteriormente al regularizar, tanto con Ridge como con Lasso nuestro objetivo ha sido el de limitar los **grados de libertad** que tiene la Regresi칩n Lineal agregando condiciones a los valores posibles de los par치metros $w_j$.  \n",
    "\n",
    "Estas condiciones obligan a los valores de $w_j$ a asumir valores **m치s peque침os** que los que se hubieran obtenido con la Regresi칩n Lineal sin regularizar y es en este punto donde Lasso ofrece una caracter칤stica muy interesante:  \n",
    "\n",
    "> Lasso tiende a transformar en 0 los par치metros de las variables o features de menor relevancia de la hip칩tesis h(x) del modelo.\n",
    "\n",
    "> **Esto nos permite usar Lasso para eliminar features de escasa relavancia de nuestro Data Set para, inlcusive, aplicar luego cualquier otro modelo**.  \n",
    "\n",
    "Esto es muy importante, porque sabemos que modelos con una gran cantidad de features o variables por tener una alto grado de libertad, tienen tendencia al **overfiting**. \n",
    "\n",
    "Entonces si tenemos demasiadas features podemos pensar en dejar de usar algunas de ellas, pero la pregunta es cu치les. Bueno, Lasso nos brinda una respuesta a esa pregunta. Podemos utilizar Lasso para efectuar **Selecci칩n de Variables** de nuestro DataSet original y con el dataset depurado podemos luego aplicar **cualquier otro modelo**, como 츼rboles de Decisi칩n, kNN, SVM, etc.  \n",
    "\n",
    "Al aplicar Lasso como m칠todo de Selecci칩n de Variables hay que tener en cuenta un detalle:  \n",
    "\n",
    "- si utlizamos un valor de $\\lambda$ demasiado peque침o, los valores de los par치metros $w_j$ ser치n los mismos que con Regresi칩n Lineal sin regularizar y quiz치 niguno se haga 0.  \n",
    "\n",
    "- si, en cambio utilizamos un valor de $\\lambda$ demasiado grande, quiz치 eliminemos demasiadas variables de nuestro modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso en Scikit- Learn\n",
    "\n",
    "Igual que para Ridge, en  sklearn el hiperpar치metro  洧랝  se denomina alpha.\n",
    "\n",
    "\n",
    "La documentaci칩n oficial se encuentra en: https://scikitlearn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "El uso b치sico se ver칤a:\n",
    "~~~\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "modelo_lasso= Lasso(alpha=0.1)\n",
    "modelo_lasso.fit(X_train, y_train)\n",
    "modelo_lasso.score(X_test,y_test)\n",
    "modelo_lasso.predict(X_nuevos)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic - Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M치s all치 del bonus que nos trajo Lasso para eliminar variables de nuestro dataset, pensando ahora en mejorar la performance seguramente se habr치 preguntado qu칠 regularizaci칩n es m치s eficiente para disminuir el overfiting, la respuesta es que puede ser que la mejor respuesta sea: \"ni una ni otra, sino un poco de cada una de ellas\"; y eso es lo que es Elastic-net.\n",
    "\n",
    "Elastic - Net es simplemente una manera de utilizar ambas posibilidades de regularizaci칩n, un poco de Ridge y otro poco de Lasso, permiti칠ndonos elegir cu치nta de cada tipo deseamos. El \"truco\" se basa en lo siguiente:  \n",
    "\n",
    "En un caso como 칠ste:  \n",
    "\n",
    "Lasso + Ridge \n",
    "\n",
    "estar칤amos sumando \"todo\" Lasso  y \"todo\" Ridge , ahora f칤jes en lo siguiente:  \n",
    "\n",
    "r Lasso  + (1 - r) Ridge\n",
    "\n",
    "siendo r un valor entre 0 y 1, entonces f칤jese que si decimos \" queremos usar un 70% de Lasso\" ajustar칤amos r=0,7 y nos quedar칤a:  \n",
    "\n",
    "0,7 Lasso  + (1-0,7) Ridge\n",
    "\n",
    "es decir:  \n",
    "\n",
    "0,7 Lasso  + 0,3 Ridge\n",
    "\n",
    "Ahora estar칤amo sumando un 70% de Lasso  y un 30% de Ridge .\n",
    "\n",
    "Esto es justamente lo que se hace a la funci칩n de Costo original de OLS, J; le sumaremos de esta manera los t칠rminos de regularizaci칩n de Lasso  y Ridge.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funci칩n de Costo de ElasticNet, $J_E$ ser치 entonces:  \n",
    "\n",
    "$$ J_E = \\frac{1}{2}  \\displaystyle\\sum_{i=1}^{m} [ h(x_{i}) - y_{i}]  + r \\lambda \\displaystyle\\sum_{j=1}^{n}  |w_j| + (1-r) \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 $$  \n",
    "\n",
    "Como siempre, $J_E$ es una funci칩n de los par치metros $w_j$, y se la debe minimizar con respecto a ellos para obtener los valores 칩ptimos de los $w_j$ por ejemplo usando GD o sus derivados.\n",
    "\n",
    "\n",
    "Analicemos un poco el comportamiento del nuevo **hiperpar치metro r**:  \n",
    "\n",
    "- Si r=0, desaparecer치 el t칠rmino de Lasso y el modelo tendr칤a s칩lo regularizaci칩n Ridge. \n",
    "- Si r=1, desaparecer치 el termino de Ridge y la regularizaci칩n ser치 s칩lo Lasso.\n",
    "\n",
    "\n",
    "Los hiperpar치metros r y $\\lambda$ son configurables en Scikit - learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T19:29:22.750437Z",
     "start_time": "2020-09-03T19:29:22.747477Z"
    }
   },
   "source": [
    "### Elastic Net en Scikit- Learn\n",
    "\n",
    "\n",
    "La Documentaci칩n oficial se encuentra en: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "\n",
    "- El hiperpar치metro 洧랝 se denomina **alpha**.\n",
    "- El hiperpar치metro r se denomina **l1_ratio**\n",
    "\n",
    "\n",
    "El uso b치sico se ver칤a:\n",
    "\n",
    "~~~\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "modelo_elastic= ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "modelo_elastic.fit(X_train, y_train)\n",
    "modelo_elastic.score(X_test,y_test)\n",
    "modelo_elastic.predict(X_nuevos)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi칩n Log칤stica con Regularizaci칩n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Regularizaci칩n tambi칠n puede aplicarse a problemas de Clasificaci칩n, por ejemplo al modelo de Regresi칩n Log칤stica con la intenci칩n de disminuir el overfiting que pudiera tener nuestro modelo.  \n",
    "\n",
    "Conceptualmente la idea es exactamente la misma que la explicada anteriormente para la Regresi칩n Lineal, as칤 que simplemente recordemos c칩mo quedar칤an las ecuaciones.\n",
    "\n",
    "\n",
    "\n",
    "La funci칩n hip칩tesis para Regresi칩n Log칤stica considerando n features o variables era:  \n",
    "\n",
    "$$ h(x)= \\frac{1}{(1+e^{-(w_0+w_1 x_1 + w_2 x_2 + ...+ w_n x_n)})} $$    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funci칩n de Costo J para Regresi칩n Log칤stica considerando m observaciones era:  \n",
    "\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^{i=m} [ -y_i log(h(x_i)) - (1-y_i) log(1-h(x_i)) ] $$  \n",
    "\n",
    "que a la hora de minimizar era una funci칩n de los par치metros $w_j$ del la hip칩tesis h(x), tambi칠n podemos obviar a m o no en la expresi칩n anterior ya que el resultado ser치 el mismo. Luego la quitaremos para que queda m치s corta la expresi칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresi칩n Logistica con regularizaci칩n Ridge:\n",
    "\n",
    "El T칠rmino de Regularizaci칩n de Ridge es el mismo que antes:  \n",
    "\n",
    "$$ \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2$$ \n",
    "\n",
    "As칤 que simplemente lo sumamos a J\n",
    "\n",
    "\n",
    "$$ J_R = \\sum_{i=1}^{i=m} [ -y_i log(h(x_i)) - (1-y_i) log(1-h(x_i)) ] + \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 $$ \n",
    "\n",
    "No olvide que **no regularizamos el $w_0$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresi칩n Logistica con regularizaci칩n Lasso\n",
    "\n",
    "$$ J_L = \\sum_{i=1}^{i=m} [ -y_i log(h(x_i)) - (1-y_i) log(1-h(x_i)) ] + \\lambda \\displaystyle\\sum_{j=1}^{n}  |w_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresi칩n Log칤stica con regularizaci칩n Elastic-Net:\n",
    "\n",
    "$$ J_E = \\sum_{i=1}^{i=m} [ -y_i log(h(x_i)) - (1-y_i) log(1-h(x_i)) ] + r \\lambda \\displaystyle\\sum_{j=1}^{n}  |w_j| + (1-r) \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 $$\n",
    "\n",
    "Las consideraciones que hicimos para el valor de r son las mismas que antes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi칩n Log칤stica Regularizada en Scikit - Learn:\n",
    "\n",
    "Documentaci칩n oficial de Scikit - Learn:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "**IMPORTANTE** \n",
    "En la implementaci칩n de Scikit-Learn no utiliza $\\lambda$ = alpha, sino que utiliza justamente la inversa de $\\lambda$ denominada **C**\n",
    "\n",
    "Es decir que el par치metro $ C = \\frac{1}{\\lambda}$  y por lo tanto:  \n",
    "\n",
    "- **un valor de C peque침o corresponde a una fuerte regularizaci칩n**  \n",
    "\n",
    "- **un valor de C grande corresponde a una regularizaci칩n peque침a**.\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contenidos",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "201.225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
