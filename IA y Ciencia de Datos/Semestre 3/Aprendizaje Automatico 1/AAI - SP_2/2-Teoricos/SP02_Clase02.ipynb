{"cells":[{"cell_type":"markdown","metadata":{"toc":true,"id":"0B6Rx9Kycve5"},"source":["<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n","<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Weighted-k-Nearest-Neighbors.-Todos-los-vecinos-tiene-la-misma-importancia?\" data-toc-modified-id=\"Weighted-k-Nearest-Neighbors.-Todos-los-vecinos-tiene-la-misma-importancia?-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Weighted k-Nearest Neighbors. Todos los vecinos tiene la misma importancia?</a></span></li><li><span><a href=\"#Ejercicios:\" data-toc-modified-id=\"Ejercicios:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Ejercicios:</a></span></li></ul></div>"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-04-07T19:46:40.608289Z","start_time":"2020-04-07T19:46:40.603301Z"},"id":"5sTwU3oYcvfH"},"source":["![IES21](img/logo_ies.png)"]},{"cell_type":"markdown","metadata":{"id":"FavDiHRYcvfJ"},"source":["# SP02 Clase02: Profundizando kNN con weighted"]},{"cell_type":"markdown","metadata":{"id":"drJODonCcvfL"},"source":["##  Weighted k-Nearest Neighbors. Todos los vecinos tiene la misma importancia?"]},{"cell_type":"markdown","metadata":{"id":"hoA6eT0xcvfN"},"source":["Todo el razonamiento de k-Nearest Neighbors se basa en que suponemos que dada una nueva observación, la etiqueta pronosticada (para Clasificación) o el valor pronosticado (para Regresión) debe ser similar al de las observaciones conocidas cercanas.   \n","\n","Hasta ahora, una vez elegido un valor para k, buscamos las k observaciones conocidas más cercanas a la nueva, y si es un problema de clasificación, le asignamos la etiqueta que \"tiene más votos\" (la que aparece más veces).   \n","\n","Así por ejemplo si k fuera 5 y las etiquetas posibles fueran \"rojo\" y  \"azul\", podría ser que las etiquetas de las 5 observaciones más cercanas fueran:   \n","\n","{rojo, azul, rojo, azul, azul}   (ordenadas de más cercana a más lejanas)\n","\n","es decir:   \n","\n","rojo: 2   \n","azul: 3   \n","\n","\n","en la votación ganan las azules y se asignaría una etiqueta azul a la nueva observación. \n","\n","\n","En el caso de un problema de regresión, simplemente le asignamos como valor el promedio de los valores de esos k vecinos más cercanos, suponiendo que los valores del target  **y** de las 5 observaciones más cercanas fueran los siguientes: \n","\n","{ 10, 9, 4, 6, 2}             (ordenadas de más cercana a más lejanas)\n","\n","a la nueva observación le asignaríamos el promedio, es decir: \n","\n","$$ \\frac{10+9+4+5+2}{5} = 6$$\n","\n","Preste atención a que todas las observaciones tienen el mismo \"peso\" en estos cálculos. Esta es la forma en que kNN se utiliza habitualmente, y es la **opción por defecto** que utiliza Scikit-Learn, pero si uno desea ser más explícito en su código puede agregar el parámetro **weights='uniform'**, que es como lo hemos venido usando, de la siguiente forma:    \n"]},{"cell_type":"markdown","metadata":{"id":"Jp7iHx9OcvfR"},"source":["**Para Clasificación:**   \n","\n","modelo=neighbors.KNeighborsClassifier(n_neighbors=5,**weights='uniform'**)  "]},{"cell_type":"markdown","metadata":{"id":"gPoiOV9VcvfT"},"source":["**Para Regresión:** \n","\n","modelo = neighbors.KNeighborsRegressor(n_neighbors=5,**weights='uniform'**)  \n"]},{"cell_type":"markdown","metadata":{"id":"TRl_1SDDcvfW"},"source":["Pero también podríamos pensar que **cuánto más cercana esté una observación conocida a la nueva, más \"peso\" debería tener  y cuánto más alejada esté menor peso debería tener en la decisión**.   \n","\n","Qué pasaría si en un problema de clasificación algunas de las k observaciones conocidas están mucho más cerca a la que queremos pronosticar que las restantes ? No tendría sentido darles más **peso** a las más cercanas y menos peso a las más alejadas en nuestro sistema de **votación**?  Por ejemplo, a la más cercana podríamos darle un peso de 1 voto y a la más alejada un peso de 0,2 votos nada más. Luego sumaríamos los votos y veríamos qué etiqueta tiene más votos ponderados.\n","\n","De la misma manera si el problema fuera de regresión podríamos pensar en hacer algún tipo de **promedio ponderado por esos pesos de los valores de y** de tal forma que **las observaciones más cercanas influyeran más en el resultado a pronosticar**. En el caso de Regresión se denomina a este método:  locally weighted regression o locally weighted scatterplot\n","smoothing (abreviados como  **LOWESS o LOESS**).\n","\n","Si bien existen varias maneras de matemáticas de asignar esta ponderación SciKit-Learn nos provee sólo de una que es con el parámetro **weights='distance'** donde asigna:   \n","\n","- una ponderación mayor a las observaciones más cercanas, y,  \n","\n","- una ponderación menor cuanto más lejos se encuentra de la que queremos pronosticar,   \n","\n","es decir que es una ponderación **inversa** a la distancia.\n","\n","\n","En cuanto a la nomenclatura:"]},{"cell_type":"markdown","metadata":{"id":"vuQgFYXncvfa"},"source":["**Para Clasificación:**\n","\n","modelo=neighbors.KNeighborsClassifier(n_neighbors=5,**weights='distance'**)"]},{"cell_type":"markdown","metadata":{"id":"BDABNUtjcvfc"},"source":["**Para Regresión:**\n","\n","modelo = neighbors.KNeighborsRegressor(n_neighbors=5,**weights='distance'**)"]},{"cell_type":"markdown","metadata":{"id":"jKkyITUkcvfe"},"source":["Lamentablemente en la documentación de sklearn no pude encontrar cuál es la fórmula exacta que utiliza, pero generalmente a una distancia 0 se suele asignar un peso de 1 y luego se lo hace decrecer linealmente hasta 0 o se lo hace decrecer hiperbólicamente, también suele usarse una forma de campana de Gauss.\n"]},{"cell_type":"markdown","metadata":{"id":"cQ3dIudDcvff"},"source":["**Cuándo utilizar una ponderación uniforme o una ponderación inversa a la distancia?**  \n","\n","La mejor respuesta es que depende esencialmente de las características que tengan los datos, no hay una opción que sea **siempre** mejor que la otra.  \n","\n","Generalmente esta estrategia se recomienda:  \n","\n","- cuando se utilizan grandes valores de k, porque sino como vimos anteriormente k tendería a asignar simplemente el valor promedio de todas las observaciones.  \n","\n","- cuando las observaciones son poco densas, es decir cuando de los por ejemplo k=5 vecinos,  las dos más cercanas están realmente cerca y las 3 más lejanas estén mucho más lejos, entonces tiene sentido darles menor valor a las lejanas. \n","\n","\n","A este tipo de funciones aplicadas sobre el espacio de todas las observaciones posibles se las suele llamar **funciones kernel**, denominación que suele asustar un poco, pero que en definitiva es sencillo."]},{"cell_type":"markdown","metadata":{"id":"bgq2J8h9cvfh"},"source":["## Ejercicios:"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-04-28T18:55:22.697384Z","start_time":"2020-04-28T18:55:22.691440Z"},"id":"oxJfCJRzcvfj"},"source":["Repetir los ejercicios de kNN hechos hasta ahora, probando con el parámetro weights='distance' y comentar los resultados. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hseESqdXcvfl"},"outputs":[],"source":[""]}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Tabla de Contenidos","title_sidebar":"Contenidos","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"colab":{"name":"SP02_Clase02.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}