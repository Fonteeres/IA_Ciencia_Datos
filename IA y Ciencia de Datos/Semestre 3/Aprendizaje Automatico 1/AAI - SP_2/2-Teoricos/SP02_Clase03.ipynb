{"cells":[{"cell_type":"markdown","metadata":{"toc":true,"id":"Ph9u6iihHZUt"},"source":["<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n","<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Distancia\" data-toc-modified-id=\"Distancia-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Distancia</a></span><ul class=\"toc-item\"><li><span><a href=\"#Distancia-Euclidiana-o-L2\" data-toc-modified-id=\"Distancia-Euclidiana-o-L2-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Distancia Euclidiana o L2</a></span></li><li><span><a href=\"#Distancia-Manhattan-o-L1\" data-toc-modified-id=\"Distancia-Manhattan-o-L1-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Distancia Manhattan o L1</a></span></li><li><span><a href=\"#Distancia-de-Chebyshev\" data-toc-modified-id=\"Distancia-de-Chebyshev-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Distancia de Chebyshev</a></span></li><li><span><a href=\"#Distancia-de-Minkowski-o--L=p\" data-toc-modified-id=\"Distancia-de-Minkowski-o--L=p-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Distancia de Minkowski o  L=p</a></span></li><li><span><a href=\"#Distancia-de-Mahalanobis\" data-toc-modified-id=\"Distancia-de-Mahalanobis-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Distancia de Mahalanobis</a></span></li><li><span><a href=\"#Otras-Distancias:\" data-toc-modified-id=\"Otras-Distancias:-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Otras Distancias:</a></span></li></ul></li></ul></div>"]},{"cell_type":"markdown","metadata":{"id":"kf2fU7IgHZUy"},"source":["![IES21](img/logo_ies.png)"]},{"cell_type":"markdown","metadata":{"id":"DmmePOwhHZUy"},"source":["# Un poco de álgebra"]},{"cell_type":"markdown","metadata":{"id":"KW9yA2rQHZUy"},"source":["En los Espacios Vectoriales se define alguna (métrica) forma de medir cuando se da una definición precisa a la idea de distancia entre 2 vectores, recordemos un poco,   \n","\n"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-04-28T19:30:12.405111Z","start_time":"2020-04-28T19:30:12.401126Z"},"id":"dF6GMQB8HZUz"},"source":["## Distancia"]},{"cell_type":"markdown","metadata":{"id":"m-KKPNLzHZUz"},"source":["Sea V un espacio Vectorial, sean u, v y w vectores de V, entonces la definición de distancia debe cumplir las siguientes condiciones: (puede imaginar a u, v y w como puntos en el plano), \n","\n","Sea $ d(u,v): V -> R $  es decir que a cada par de vectores de V le asigna un número Real como resultado:\n","\n","\n","1) **No Negatividad**:  \n","\n","$$d(u,v) >=0$$    \n","\n","es decir la menor distancia entre 2 puntos es cero, o sea que no existen distancias negativas.  \n","\n","\n","2) **Identidad**:  \n","\n","$$ d(u,v) = 0 <=> u=v$$  \n","\n","es decir que la única posibilidad para que la distancia entre 2 puntos sea cero, es que ambos puntos sean el mismo punto!\n","\n","3) **Simetría**:  \n","\n","$$ d(u,v) = d(v,u)$$\n","\n","es decir la distancia entre u y v es la misma que entre v y u.   \n","\n","4) **Desigualdad del triángulo**: \n","\n","$$ d(u,v) + d(v,w) > = d(u,w) $$  \n","\n","\n","que como se observa en la figura siguiente sólo dice que el lado rojo del triángulo es más corto (o a lo sumo igual) que la suma de los otros dos lados: \n","\n","![Desigualdad del Triángulo](img/Desigualdad_triangulo.png)  \n","\n","(La posibilidad de la igualdad es cuando los 3 puntos están sobre una misma recta, pero digamos que no formarían un triángulo).\n","\n","En definitiva cualquier operación que hagamos con dos vectores que dé por resultado un número real y que cumpla estas 4 condiciones **es** una distancia y la podemos usar para medir y para ver qué tan cercanos o lejanos están nuestros vectores de features."]},{"cell_type":"markdown","metadata":{"id":"qXipr10QHZU0"},"source":["Como en kNN y trambén en otros modelos se hace uso del concepto de distancia, veamos cuáles son los más usuales, cómo se encuentran en Scikit-Learn y cómo se los suele denominar.  \n","\n","Cuando sea posible, ejemplificaremos en $ V = R^2$ que es donde tenemos intuición geométrica.\n","\n"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-04-28T18:58:17.668277Z","start_time":"2020-04-28T18:58:17.662318Z"},"id":"oi2JENLgHZU1"},"source":["Llamemos $ u = (u_1, u_2,...,u_n) $, y $ v = (v_1, v_2,...,v_n) $  los dos vectores:"]},{"cell_type":"markdown","metadata":{"id":"IWYm6O75HZU1"},"source":["### Distancia Euclidiana o L2\n","\n","![Distancia Euclidiana](img/Distancia_Euclidiana.png)  \n","\n","la Distancia Euclidiana es la que estamos acostumbrados a considerar cuando hablamos de **\"distancia\"**, es decir la longitud del segmento de recta que une los puntos u y v.   \n","\n","Dado que conocemos las coordenadas de u y v, podemos fácilmente calcular cuánto mide cada cateto y luego aplicando Pitágoras: \n","\n","$$ d(u,v) = \\sqrt{(u_1-v_1)^2 + (u_2-v_2)^2} $$   \n","\n","por supuesto si pensamos en vectores de n componentes simplemente extendemos la fórmula:  \n","\n","$$ d(u,v) = \\sqrt{(u_1-v_1)^2 + (u_2-v_2)^2 +...+(u_n-v_n)^n} $$  \n","\n","lo que podríamos compactar con la nomenclatura de sumatoria: \n","\n","$$d(u,v) = \\sqrt{\\sum_{i=1}^{i=n}{(u_i-v_i)^2} }$$  \n","\n","**En Scikit-Learn utilizar**:  \n","\n","No es necesario indicarle ninguna métrica, ya que es la que trae por defecto, pero si lo necesitara por algún motivo:  \n","\n","\n","> sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', **metric='euclidian'**, metric_params=None, n_jobs=None, **kwargs)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ir9aqmwIHZU2"},"source":["### Distancia Manhattan o L1\n","\n","![Distancia Manhattan](img/Distancia_Manhattan.png)  \n","\n","La distancia euclidiana puede ser útil para los pájaros pero no para los taxistas, quienes se mueven en la cuadrícula de calles de una ciudad como Manhattan, para ellos puede ser más util medir la distancia entre 2 puntos siguiendo trazos paralelos a cada uno de los ejes coordenados, como los indicados en rojo en la figura anterior.\n","\n","Basándonos en la figura anterior:  \n","\n","$$ d(u,v) = |(u_1-v_1)| + |(u_2-v_2)| $$\n","\n","observe que pusimos la longitud de cada uno de los tramos en valor absoluto, porque esperamos que éstos sean número siempre positivos. \n","\n","\n","$$d(u,v) = \\sqrt{\\sum_{i=1}^{i=n}{|u_i-v_i|} }$$\n","\n","**En Scikit-Learn utilizar**:  \n","\n","> sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', **metric='manhattan'**, metric_params=None, n_jobs=None, **kwargs)\n"]},{"cell_type":"markdown","metadata":{"id":"kQsT0icbHZU3"},"source":["### Distancia de Chebyshev\n","\n","Podemos pensarla en función del gráfico anterior: cuál de los dos segmentos tiene mayor longitud? A mí me parece que el horizontal, el que mide $(u_1 - v_1)$, bueno, ésa sería justamente la distancia de Chebyshev!.\n","\n","$$ max( (u_1-v_1), (u_2-v_2))) $$  \n","\n","o en general para los n segmentos:  \n","\n","$$ max( |u_1-v_1|, |u_2-v_2|, ...,|u_n-v_n|) $$ \n","\n","o en nomenclatura más abreviada:  \n","\n","$$ max( |u_i-v_i|) $$\n","\n","\n","\n","\n","**En Scikit-Learn utilizar**:  \n","\n","> sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', **metric='chebyshev'**, metric_params=None, n_jobs=None, **kwargs)\n"]},{"cell_type":"markdown","metadata":{"id":"T8dsxHUsHZU3"},"source":["### Distancia de Minkowski o  L=p\n","\n","Podemos pensar a la Distancia de Minkowski como una **generalización** de la Euclidiana. En este caso no hacemos una interpretación gráfica, así que la fórmula sería:  \n","\n","$$ d(u,v) = \\sqrt[p]{(u_1-v_1)^p + (u_2-v_2)^p} $$  o en forma general para cualquier dimensión:  \n","\n","$$ d(u,v) = \\sqrt[p]{(u_1-v_1)^p + (u_2-v_2)^p +...+(u_n-v_n)^p} $$\n","\n","o bien con sumatoria: \n","$$d(u,v) = \\sqrt[p]{\\sum_{i=1}^{i=n}{(u_i-v_i)^p} }$$  $$ [\\sum_{i=1}^{i=n}{(u_i-v_i)^p)]^{\\frac{1}{p}}} $$\n","\n","donde podemos observar que la distancia euclidiana sería simplmente una de Minkowski con valor p=2. Generalmente se utilizan los valores de p pares.\n","\n","**En Scikit-Learn utilizar**, por ejemplo para p=4\n","\n","> sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, **p=4**, **metric='minkowski'**, metric_params=None, n_jobs=None, **kwargs)  \n","\n","\n","- Observe que la distancia **euclidiana** también podría escribirse como una de Minkowski con p=2 \n","- La distancia **Manhattan**, también podría escribirse como una de Minkowski pero esta vez con p=1.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-04-28T20:47:53.666338Z","start_time":"2020-04-28T20:47:53.660351Z"},"id":"TbhzpME6HZU3"},"source":["### Distancia de Mahalanobis"]},{"cell_type":"markdown","metadata":{"id":"jjp_JMOQHZU4"},"source":["Esta es una forma de calcular la distancia que puede verse como una mejora de la euclidiana pero que tiene en cuenta el rango de valores en que pudieran estar los valores de cada eje (cosa que podemos mejorar antes, si normalizamos previamente los datos, ya hablaremos de ello). En este caso nos quedamos sólo con el concepto.  \n","\n","**En Scikit-Learn utilizar**:  \n","\n","> sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', **metric='mahalanobis'**, **metric_params={'V': np.cov(X)}**, n_jobs=None, **kwargs)\n"]},{"cell_type":"markdown","metadata":{"id":"CWldbB2bHZU4"},"source":["### Otras Distancias: \n","\n","- Hay más distancias definidas en Scikit-learn, pero hay algunas interesantes , como la **distancia coseno**, que se mide como el **coseno del ángulo formado entre los dos vectores** que no están definidas en Scikit-Learn, así que las veremos cuando particularmente las necesitemos. Afortunadamente sklearn permite crear nuestras propias fórmulas de distancia."]}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Tabla de Contenidos","title_sidebar":"Contenidos","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"colab":{"name":"SP02_Clase03.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}