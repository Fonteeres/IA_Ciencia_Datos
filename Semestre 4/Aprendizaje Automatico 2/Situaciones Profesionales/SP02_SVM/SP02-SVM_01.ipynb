{"cells":[{"cell_type":"markdown","metadata":{"id":"OYO7EJSZDz8e"},"source":["# SVM: Support Vector Machines"]},{"cell_type":"markdown","metadata":{"id":"S7wXtkFGDz8f"},"source":["## Introducción a SVM"]},{"cell_type":"markdown","metadata":{"id":"potRfC9tDz8g"},"source":["SVM es un modelo de Machine Learning **muy potente**, en un principio cuando surgió parecía que iba a ser la panacea de los modelos de ML; si bien no es así, podemos afirmar que es muy potente y además sumamente versátil por lo cual lo veremos aplicado en diversas áreas y se constituye en uno de los modelos más utilziados.\n","\n","Si bien se lo suele aplicar más que nada a los problemas de **Clasificación**, también se lo ha adaptado para utilizarlo en problemas de **Regresión**.  \n","\n","En los casos de Clasificación es capaz de generar Fronteras de Decisión **lineales**, es decir que por ejemplo dicha frontera puede ser una línea recta (en un caso con 2 features) o un plano en el caso de 3 features, pero también puede generar Fronteras de Decisión **no lineales** es decir que su gráfico en el caso de 2 features sería una curva o una superficie curva en el caso de 3 features.   \n","\n","Generalmente se lo suele utilizar con **datasets no demasiado grandes**, aunque veremos que Scikit-Learn incluye alguna implementación que mejora su performance: se puede usar Stochastic Gradient Descent, SGD, para minimizar su función de Costo. \n","\n","- A diferencia de Regresión Lineal, SVM **no proporciona las probabilidades de que una nueva observación pertenezca a una u otra clase**.\n"]},{"cell_type":"markdown","metadata":{"id":"d9TBJAwFDz8g"},"source":["**Importante**:  \n","\n","> SVM se ve particularmente afectado por las distintas escalas que pueden tomar las variables, así que se recomienda **estandarizarlas** o **normalizarlas** si es necesario, antes de aplicar SVM. \n"]},{"cell_type":"markdown","metadata":{"id":"EaqfJvG9Dz8h"},"source":["## Fundamentos de SVM para Clasificación lineal (frontera de decisión lineal)"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-08-21T13:52:09.044229Z","start_time":"2020-08-21T13:52:09.038261Z"},"id":"DaRvQiQGDz8h"},"source":["Para ejemplificar, supongamos que tenemos un problema de clasificación con dos variables o features, $x_1$ y $x_2$ y una variable  a pronosticar, $y$, binaria (que asume sólo dos categorías) que mostraremos con dos colores distintos en el gráfico.  \n","\n","Supongamos también, por ahora, que ambas clases son linealmente separables, es decir que las podemos separar perfectamente con una línea recta como se muestra en la figura:"]},{"cell_type":"markdown","metadata":{"id":"ADZKEoQoDz8h"},"source":["![SVM_01.png](https://i.ibb.co/KbJ2zbq/SVM-01.png)"]},{"cell_type":"markdown","metadata":{"id":"0oBbjixgDz8i"},"source":["Podemos imaginar *infinitas* rectas capaces de separar las observaciones que corresponden a ambas clases:"]},{"cell_type":"markdown","metadata":{"id":"EhwcfPXYDz8i"},"source":["![SVM_02.png](https://i.ibb.co/vHSL4QS/SVM-02.png)"]},{"cell_type":"markdown","metadata":{"id":"vIaHC9SCDz8j"},"source":["- r1 *daría la impresión de estar demasiado cerca* de los puntos rojos.  \n","- Una recta como r3 podría corresponder a la estrategia que siguen los Árboles de Decisión (separa con rectas paralelas a cada eje coordenado, como ya hemos mencionado varias veces).  \n","- r4 *daría la impresión de pasar demasido cerca* de algunos rojos y algunos verdes."]},{"cell_type":"markdown","metadata":{"id":"lAM0bVCWDz8j"},"source":["En SVM la estrategia es la siguiente:  \n","\n","- Buscar los dos puntos de cada clase **más cercanos entre sí** en la siguiente figura los identificamos como A y B."]},{"cell_type":"markdown","metadata":{"id":"a2y_tomSDz8j"},"source":["![SVM_03.png](https://i.ibb.co/G9DtD16/SVM-03.png)"]},{"cell_type":"markdown","metadata":{"id":"G_hS-6DrDz8k"},"source":["- Y determinar la recta perpendicular al segmento que los une, pasando por el punto medio del mismo.  Como se muestra en la siguiente figura:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xSQugaioDz8k"},"source":["![SVM_04.png](https://i.ibb.co/m8VRj1k/SVM-04.png\n",")"]},{"cell_type":"markdown","metadata":{"id":"mcJ2fdYHDz8k"},"source":["De esta manera esta recta se encuentra a la máxima distancia posible de los dos puntos más cercanos. Si estuviera más a la izquierda estaría más cerca de los puntos rojos y si estuviera más a la derecha estaría más cerca de los cuadrados verdes. \n","\n","\n","- SVM determina la **\"avenida\" más ancha posible entre las dos clases**, en la siguiente figura hemos marcado la \"avenida\" con las rectas punteadas.   \n","\n","\n","- Por eso se dice que SVM es un **Clasificador de Amplio Margen (Large Margin Classificator)**."]},{"cell_type":"markdown","metadata":{"id":"uapOrps9Dz8l"},"source":["![SVM_05.png](https://i.ibb.co/LrDd1gw/SVM-05.png\n",")"]},{"cell_type":"markdown","metadata":{"id":"prJNcurHDz8l"},"source":["Como **r queda totalmente determinada sólo por los puntos (o vectores) A y B** se dice que ellos son los **vectores de \"apoyo\" o soporte** (**support vectors**) del modelo, de allí el nombre SVM."]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-09-10T16:34:04.627002Z","start_time":"2020-09-10T16:34:04.623014Z"},"id":"bXPAzUjNDz8l"},"source":["### Cómo clasifica nuevas observaciones SVM?"]},{"cell_type":"markdown","metadata":{"id":"owNuoUpwDz8l"},"source":["Al entrenar a SVM con las observaciones del Train Set, descubre los support vectors A y B y en función de ellos calcula la ecuación de la recta (para el caso lineal de 2 variables, sino en general sería un hiperplano. Más adelante veremos que la frontera puede ser también una curva o superficies curvas cuando la dimensión es superior) que brinda el margen de separación más amplio posible entre las dos clases.   \n","\n","El entrenamiento puede llevar cierto tiempo, pero una vez determinada la ecuación de la recta, la aplicación de la misma es muy veloz, es decir que SVM se comporta bien en producción.  \n","\n","Cuando llegue una nueva observación el modelo sólo deberá determinar de cuál de los dos lados de la recta se encuentra y asignará la clase que corresponde."]},{"cell_type":"markdown","metadata":{"id":"sjg4S3hbDz8l"},"source":["- El criterio para  clasificar una nueva observación en ejemplo anterio, sería que **si un nuevo punto se encuentra a la izquierda de r, se le asignaría la clase roja y si estuviera a la derecha sería considerado verde**.  "]},{"cell_type":"markdown","metadata":{"id":"us2caD_mDz8l"},"source":["![SVM_05b.png](https://i.ibb.co/YbYsByv/SVM-05b.png\n",")"]},{"cell_type":"markdown","metadata":{"id":"rC2CjkV9Dz8m"},"source":["Así, por ejemplo, en la figura anterior las nuevas observaciones C y D se clasificarían como rojas y las E y F como verdes. "]},{"cell_type":"markdown","metadata":{"id":"c4MxR7-lDz8m"},"source":["### Hard Margin SVM"]},{"cell_type":"markdown","metadata":{"id":"WD3pAyPeDz8m"},"source":["Se denomina Hard Margin SVM cuando se exige que la \"avenida\" dada por el margen de separación tiene que estar **vacía** y que además todas **las observaciones deben estar del lado correcto de la recta frontera de decisión**. Es decir un caso como el siguiente sería lo ideal:  \n"]},{"cell_type":"markdown","metadata":{"id":"ZbosmEf0Dz8m"},"source":["![SVM_05.png](https://i.ibb.co/LrDd1gw/SVM-05.png\n",")"]},{"cell_type":"markdown","metadata":{"id":"fTJ6kTD_Dz8m"},"source":["Pero imponer condiciones tan estrictas, puede traer dos tipos de inconvenientes. Por ejemplo qué pasaría si tuviéramos algún outlier que estuviera \"del lado que no le corresponde\"?  "]},{"cell_type":"markdown","metadata":{"id":"HEermZGHDz8m"},"source":["![SVM_07.png](https://i.ibb.co/0Z8wmKc/SVM-07.png\n",")"]},{"cell_type":"markdown","metadata":{"id":"1ldXJw88Dz8m"},"source":["Lamentablemente el método fallaría ya que es imposible separar linealmente las clases."]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-09-10T15:50:40.383625Z","start_time":"2020-09-10T15:50:40.377641Z"},"id":"A6cZxZNoDz8m"},"source":["El otro inconveniente que podríamos tener con algún outlier podría ser el sigiente:  "]},{"cell_type":"markdown","metadata":{"id":"OTfBaY1ODz8n"},"source":["![SVM_08.png](https://i.ibb.co/bdb9hP0/SVM-08.png)"]},{"cell_type":"markdown","metadata":{"id":"NtJm0ZyMDz8n"},"source":["En este caso, si bien sería posible separar las clases con una recta, la presencia del outlier la está corriendo demasiado hacia la derecha y posiblemente el modelo no generalice muy bien.   \n","\n","En la realidad estas situaciones existen y es por eso que es preferible tener cierto grado de tolerancia (atención ... hiperparámetro!) dando lugar al Soft Margin Classification..."]},{"cell_type":"markdown","metadata":{"id":"F3teAHk8Dz8n"},"source":["### Soft Margin SVM"]},{"cell_type":"markdown","metadata":{"id":"V_HOL3LhDz8n"},"source":["El objetivo es **mantener el márgen más amplio posible** para lo cual deberemos **tolerar un cierto grado de violaciones a las reglas** tan estrictas planteadas anteriormente, como se muestra en la siguiente figura donde hemos tolerado la presencia de algunas observaciones del \"lado incorrecto\":"]},{"cell_type":"markdown","metadata":{"id":"vlrZbyEvDz8n"},"source":["![SVM_09.png](https://i.ibb.co/1Gmbrb8/SVM-09.png)"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-09-10T16:23:03.905772Z","start_time":"2020-09-10T16:23:03.900761Z"},"id":"ZAY2ZDVrDz8n"},"source":["Un modelo como éste con un amplio margen, seguramente generalizará mejor que el anterior con un margen tan estrecho.  "]},{"cell_type":"markdown","metadata":{"id":"6uq12somDz8n"},"source":["#### Hiperparámetro C"]},{"cell_type":"markdown","metadata":{"id":"q2tpTI4eDz8n"},"source":["El hiperparámetro **C regula el grado de rigidez de las reglas** y se suele considerar que es también una **Regularización**, C es positivo, **C>0** y:  \n","\n","- Si **C es grande**: \n","    - las reglas serán muy **estrictas**, \n","    - el margen de **separación será menor** y \n","    - habrá propensión al overfiting.  \n","    \n","    \n","- Si **C es pequeño**:  \n","    - las reglas serán más **laxas**, \n","    - el margen de **separación será más amplio**\n","    - habrá propensión al underfiting.\n","    \n","    \n","Por supuesto esto nos indica que:\n","\n","> **Si estamos en presencia de overfiting, disminuir C** y viceversa, si estamos en presencia de underfiting, aumentarlo."]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-09-10T16:44:32.543664Z","start_time":"2020-09-10T16:44:32.540696Z"},"id":"b2SvqigUDz8o"},"source":["## SVM en Scikit-Learn"]},{"cell_type":"markdown","metadata":{"id":"jkcHvR9yDz8o"},"source":["Existen varias formas de aplicar SVM con sklearn, podemos usar las siguientes clases:\n","\n","- [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html): \n"," * El tiempo de ajuste **escala al menos cuadráticamente con el número de muestras** y puede no ser práctico más allá de decenas de miles de muestras\n"," * Clasificación por esquema de **uno contra uno**\n","\n","\n","- [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html):\n"," * Tiene más flexibilidad en la elección de penalizaciones y funciones de pérdida y debería **escalar mejor a un gran número de muestras**\n"," * Admite **entradas densas y dispersas** \n"," * Clasificación por esquema de **uno contra el resto (ovr)**\n"," \n","\n","\n","- [sklearn.linear_model.SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html):\n"," * El modelo al que se ajusta se puede controlar con el parámetro de pérdida (loss); por defecto, se ajusta a un modelo de SVM.\n"," * Para obtener los mejores resultados con el learning_rate predeterminado, los datos deben tener una **media cero y una varianza unitaria**\n"," * Admite **entradas densas y dispersas** \n","\n","*nota: SVC significa \"Support Vector Classification\"*"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-09-10T19:24:43.326327Z","start_time":"2020-09-10T19:24:43.322335Z"},"id":"0PzHObmyDz8o"},"source":["### LinearSVC"]},{"cell_type":"markdown","metadata":{"id":"8_-84RpYDz8o"},"source":["Documentación oficial: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n","\n","~~~\n","sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n","~~~\n","\n","- Es **rápido**, posiblemente el más veloz de los 3.\n","- **penalty**: admite penalización l1 o l2, por defecto utiliza l2 y es el más usado, así que generalmente no se cambia.\n","- **loss**: Permite elegir la función de Costo. La estándard es \"hinge\", pero por defecto selecciona \"squared hinge\", se pude utilizar \"hinge\"\n","- **dual=True** **Si hay más variables que observaciones dejar en True**, en cambio si hay más observaciones que variables, setear en False.\n","- **C:1** ya hemos hablado de C.\n","- **multi_class='ovr'**. Si la variable y es binaria, este parámetro se ignora. Si la variable y es multiclase entonces utilizar ovr (One Vs Rest) que es el valor por defecto. Así que conviene dejarla como está.\n","- **fit_intercept=True**. Si es True, entonces calculará el término independiente sino, asumirá que es 0. Si uno previamente estandarizó, entonces conviene ponerlo en False.\n","- **class_weight=None**. Por defecto considera que el valor de C corresponde por igual a todas las variables. Es posible pasar un valor distinto para cada variable o utilizar ‘balanced’ que automáticamente los genera. Generalmente se lo deja por defecto. Ver la documentación para más info.   \n","- **random_state=None**. Si el problema es bivariado, no afecta, pero cuando el problema es multiclass el parámetro dual=True, ciertos valores se seleccionan al azar, en ese caso podemos pasar un valor de semilla para ganar repetibilidad.\n"]},{"cell_type":"markdown","metadata":{"id":"H2arjAJPDz8o"},"source":["~~~\n","from sklearn.svm import LinearSVC\n","\n","modelo= LinearSVC(C=1, loss=\"hinge\")\n","modelo.fit(X_train, y_train)\n","modelo.predict(X_test)\n","\n","modelo.score(X_test, y_test)           # devuelve la Accuracy media.\n","~~~"]}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Tabla de Contenidos","title_sidebar":"Contenidos","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"165px"},"toc_section_display":true,"toc_window_display":true},"colab":{"name":"SP02-SVM_01.ipynb","provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}