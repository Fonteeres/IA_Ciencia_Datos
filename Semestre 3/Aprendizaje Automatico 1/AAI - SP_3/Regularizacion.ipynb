{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Lineales con Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:07:46.932981Z",
     "start_time": "2020-09-03T21:07:46.929505Z"
    }
   },
   "source": [
    "### El problema del Overifiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los problemas que solemos enfrentar es el **overfiting**, el cual se produce cuando nuestro modelo es \"demasiado complejo\" y comienza a aprender de casos particulares del Train Set y  del ruido que tenemos en nuestras observaciones, pareciéndose *demasiado* a las particularidades de los datos del Train Set y por lo tanto teniendo una baja performance en la generalización al medir el Error en el Test Set.   \n",
    "\n",
    "Resumiendo, **nuestro modelo tiene overfiting cuando anda muy bien en el Train Set y anda mal al pronosticar en el Test Set**.\n",
    "\n",
    "> Los métodos de Regularización Ridge, Lasso y Elastic - Net nos permitirán crear modelos con menor **overfiting**. Tipicamente se aplican para los modelos de Regresión Lineal y Regresión Logística ya que se basan en modificar los valores que se obtendrían de los parámetros de estos modelos.\n",
    "\n",
    "Pensemos en un caso conocido por nosotros como es el de Regresión Lineal, con una variable $x$ predictora y una variable $y$ que deseamos pronosticar. En ese caso nuestra hipótesis podía tomar la forma más sencilla:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x $$  \n",
    "\n",
    "La cual podíá ser útil si el gráfico de y(x) tenía aspecto de una recta. Pero si su aspecto era distinto entonces probábamos con polinomios de distinto grado:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x + w_2 x^2 + ... + w_n x^n$$  \n",
    "\n",
    "Pero al aumentar el grado de polinomio, nuestra h(x) comienza a adquirir la posibilidad de subir y bajar con un mayor *grado de libertad* y por lo tanto  a \" adapatarse\" demasiado bien a los puntos individuales del Train Set, de tal manera que comenzaba a de casos particulares dentro del Train y también a \"aprender\" del ruido y puede generalizar mal.   \n",
    "\n",
    "El exceso de parámetros $w_i$ de nuestra hipótesis es lo que brinda el **grado de libertad** de nuestro modelo.  \n",
    "\n",
    "Es decir que: \n",
    "\n",
    "> Si tenemos *demasidos* parámetros $w_i$ nuestra hipótesis pasa a tener demasiados *grados de libertad* y aumenta la tendencia al overfitting.  \n",
    "\n",
    "Ahora bien, ésto no ocurre solamente cuando h(x) corresponde a un polinomio de potencias de x, también ocurrirá cuando tengamos **muchas features**, particularmente si algunas de ellas no son muy explicativas del valor de y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que contamos en nuestro dataset con n features = $ {x_1,x_2, ...,x_n}$ y la variable numérica $ y $ que deseamos pronosticar. Nuestra hipótesis podría ser de la forma:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n $$  \n",
    "\n",
    "Si la cantidad de variables $x_i$ es excesiva nuestro modelo podrá desarrollar overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Una de las maneras de intentar disminuir el overfitting es utilizar menos features, la pregunta sería entonces, cuáles no usar.   \n",
    "\n",
    "- Existen métodos para ver cuáles de las features nos brindan poca o nula información con respecto a la variable $y$ que queremos pronosticar (y de esta manera eliminarlas de nuestro dataset). Pero también existe otro método que nos permite reducir el overfitting, la **Regularización**, y quizá no necesitemos quitar demasiadas features que tangan valor explicativo para nuestro modelo.  \n",
    "\n",
    "La idea central para el concepto de regularización consiste en ampliar un poco la idea de los *grados de libertad*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pogamos por caso una hipótesis como la polinómica, porque es más fácil de pensar, pero lo mismo serviría cuando las features no son potencias de una variable:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x + w_2 x^2 + ... + w_n x^n$$    \n",
    "\n",
    "Dijimos que si n era demasiado grande, un polinomio de grado alto puede adaptarse prácticamente a cualquier conjunto de puntos que querramos, lo cual generalizamos ahora diciendo que tiene un alto grado de libertad.  \n",
    "\n",
    "Ahora bien qué pasaría si imponemos la condición de que $w_n = 0$?  Indudablemente el polinomio ahora tendría menor potencia y por ende menor grado de libertad.   Uno puede pensar que es el mismo efecto que haber quitado directamente a $x^n$, lo cual es cierto porque le asignamos un valor de 0 ... pero también le quitaríamos libertad si le exigimos que tome cualquier otro valor, como por ejemplo $ w_n = 4$!  \n",
    "\n",
    "Al restringir el valor de alguno de los parámetros, le quitamos grados de libertad a la expresión polinómica!  \n",
    "\n",
    "Es cierto que restringir el valor de alguno de los parámetros $w_i$ es un caso un poco extremo ... pero podríamos hacer algo más sutil ... si el problema es porque el polinomio tiene \"potencias demasiado altas\", en vez de quitarlas directamente asignándoles $w_n=0$ podemos hacer algo que no la haga desaparecer, pero que *atenúe* su influencia frente a las demás ... por ejemplo podemos imponer la condición de que:  \n",
    "\n",
    "$$w_i < 0.1$$  \n",
    "\n",
    "de esta manera no hacemos desaparecer la potencia n del polinomio, pero es como si lo transformáramos en uno más restringido, casi como pensar que es un polinomio con un grado *algo* menor pero que no llega a ser una potencia menor!  \n",
    "\n",
    "En el caso específico de la expansión polinómica seguramente pensaríamos en reducir el valor de los $w_i$ para las potencias más altas, ahora bien, la idea es totalmente aplicable al caso más general cuando nuestra hipótesis tienen muchos términos porque contamos con muchas features sin relación entre ellas:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n $$    \n",
    "\n",
    "Excepto que en este caso, para quitar grados de libertad sin quitar variables le imponemos la condición de que **todos los valores de $w_i$ deban ser pequeños**.   \n",
    "\n",
    "> La idea central en la Regularización de modelos lineales (como Regresión Lineal e inclusive Regresión Logística) pasa por **disminuir los grados de libertad de nuestra hipótesis, la cual conseguiremos al restringir los valores que pueden tomar los parámetros $w_i$**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de los modelos con regularización:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque efectuaremos cambios en algunas partes del modelo de Regresión Lineal, \n",
    "\n",
    "> las métricas de evaluación **no cambian**, segurián siendo las mismas de siempre: MSE, RMSE, R<sup>2</sup>, etc\n",
    "\n",
    "usadas habitualmente para los problemas de Regresión y lo mismo sucederá para los modelos de Clasificación con Regularización.\n",
    "\n",
    "**Las métricas de evaluación no dependen del modelo elegido, aún para los próximos modelos de ML que veremos a lo largo de la carrera.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T18:30:19.808369Z",
     "start_time": "2020-09-01T18:30:19.802386Z"
    }
   },
   "source": [
    "## Ridge (también se lo conoce como Tikhonov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo Ridge restringe (a veces se dice  **penaliza**) los valores de los parámetros $w_i$ agregando un término extra en la función de Costo J de la Regresión Lineal habitual.  \n",
    "\n",
    "La hipótesis para Regresión Lineal con $n$ features o variables es:  \n",
    "\n",
    "$$ h(x) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n $$  \n",
    "\n",
    "  \n",
    "\n",
    "Suponiendo que tengamos $m$ observaciones en nuestro Train Set, la función de Costo J habitual para Regresión Lineal es la que utiliza mínimos cuadrados u OLS (Ordinary Least Squares), aclaremos que se podría usar cualquier otra, pero dado que es la más usada desarrollemósla:\n",
    "\n",
    "$$ J = \\frac{1}{2m}  \\displaystyle\\sum_{i=1}^{m} [ h(x_{i}) - y_{i}]²  $$     \n",
    "\n",
    "dado que en m es la cantidad de observaciones, lo cual es una constante, algunas implementaciones la quitan de la fórmula de J, ya que el resultado será el mismo. Nosotros la quitaremos para que queda más sencilla la expresión:\n",
    "\n",
    "\n",
    "$$ J = \\frac{1}{2}  \\displaystyle\\sum_{i=1}^{m} [ h(x_{i}) - y_{i}]²  $$     \n",
    "\n",
    "\n",
    "\n",
    "o lo que es lo mismo, poniendo de manifiesto que J es una función de los w:\n",
    "\n",
    "$$ J = \\frac{1}{2}  \\displaystyle\\sum_{i=1}^{m} [w_0 + w_1 x_{1i} + w_2 x_{2i} + ... + w_n x_{ni}  - y_{i}]²  $$\n",
    "\n",
    "Recordemos que debemos **minimizar** J con respecto a los parámetros $w$ para así determinar la hipótesis h(x) que mejor aproxima a las m observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En Ridge agregamos un término más a la función de Costo, el **Término de Regularización de Ridge**, para restringir los valores que pueden tomar los parámetros $w_j$ y de esta manera **disminuir los grados del libertad** que tiene el modelo y de esta manera evitar el **overfitting**:\n",
    "\n",
    "$$  \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 $$    \n",
    "\n",
    "Como se basa en la suma de los cuadrados (potencia 2) de los parámetros, se suele decir que es una **penalización $l_2$**.  \n",
    "\n",
    "$\\lambda$ es un nuevo hiperparámetro y asume sólo valores positivos.\n",
    "\n",
    "Al sumarlo a la función de Costo J, la nueva función de Costo queda de la siguiente forma:  \n",
    "\n",
    "$$ J_R = J+ \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2$$   \n",
    "\n",
    "es decir:\n",
    "\n",
    "\n",
    "$$ J_R = \\frac{1}{2} ( \\displaystyle\\sum_{i=1}^{m} [ h(x_{i}) - y_{i}]²  + \\lambda \\displaystyle\\sum_{j=1}^{n}  w_j^2)$$  \n",
    "\n",
    "Pasando a ser esta nueva función de Costo $J_R$ la que deberemos minimizar con respecto a los parámetros $w_j$.  \n",
    "\n",
    "Nota: El proceso de minimización puede ser Gradient Descent y sus derivados pero también existen formas matriciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un poco el nuevo **Término de Regularización de Ridge**:  \n",
    "    \n",
    "$$  \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 $$    \n",
    "\n",
    "donde $  \\lambda > 0$  es un hiperparámetro para la regularización.\n",
    "\n",
    "En definitiva es: \n",
    "\n",
    "$$\\frac{\\lambda}{2} (w_1^2 + w_2^2 + w_3^2 ... + w_n^2)$$  \n",
    "\n",
    "Observemos que por ser una suma de números al cuadrado, su valor siempre será positivo o a lo sumo 0. \n",
    "\n",
    "$$\\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 \\geq 0$$\n",
    "\n",
    "\n",
    "\n",
    "$n$ es la cantidad de features o variables de nuestro modelo lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos que:  \n",
    "    \n",
    "- $w_0$ **no**  forma parte del Término de Ridge, es decir no restringimos el valor que puede tomar.  \n",
    "\n",
    "- Al agregar este término, como estarán sujetos a la minimización de $J_R$, los $w_1,w_2 ..., w_n$ no podrán tomar cualquier valor, sino que deberán mantenerse pequeños para que $J_R$ sea minimizada.\n",
    "\n",
    "- $\\lambda$ es un valor positivo,  es un **hiperparámetro** de Regularización y sirve para determinar \"cuánta regularización\" queremos que tenga el modelo:  \n",
    "\n",
    "    - Si $ \\lambda$ es = 0, $J_R$ sería igual que la $J$ original, así que el modelo **no tendría Regularización**\n",
    "    - Si $ \\lambda$ es un valor *grande*, entonces como todo el término de Regularización:\n",
    "    \n",
    "    $$\\frac{\\lambda}{2} (w_1^2 + w_2^2 + w_3^2 ... + w_n^2)$$\n",
    "    \n",
    "    tiene que ser pequeño, deberá ocurrir que   $$(w_1^2 + w_2^2 + w_3^2 ... + w_n^2)$$   \n",
    "    sea *muy pequeño*, lo cual significa que le estaremos exigiendo una **gran regularización**.  \n",
    "    \n",
    "En definitiva:   \n",
    "\n",
    "- si elegimos un valor de $\\lambda$ pequeño el modelo tendrá poca regularización\n",
    "- si elegimos un valor de  $\\lambda$ grande el modelo tendrá mucha regularización\n",
    "\n",
    "Al modelizar, deberemos ajustar el valor de $\\lambda$ probando con varios de ellos para determinar con qué nivel de regularización obtenemos mejores resultados, por ejemplo con el valor del RMSE.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T18:58:13.354910Z",
     "start_time": "2020-09-03T18:58:13.351920Z"
    }
   },
   "source": [
    "### Ridge en Scikit - Learn\n",
    "\n",
    "Antes de aplicar, es preferente estandarizar los datos.\n",
    "\n",
    "Recordemos que en Scikit-Learn la sintaxis es muy coherente para cualquier modelo que deseemos utilizar, en esencia:  \n",
    "\n",
    "- instanciar el modelo\n",
    "- .fit\n",
    "- .predict\n",
    "- .score\n",
    "\n",
    "En sklearn el **hiperparámetro $\\lambda$ se denomina alpha**\n",
    "\n",
    "En el caso de Ridge, la librería que debemos importar es: sklearn.linear_model.Ridge, la documentación oficial se encuentra aquí:  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "\n",
    "El uso básico se vería:\n",
    "\n",
    "~~~\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "modelo_ridge=Ridge(alpha=0.25)\n",
    "modelo_ridge.fit(X_train,y_train)\n",
    "modelo_ridge.score(X_test,y_test)\n",
    "modelo_ridge.predict(X_nuevos)\n",
    "~~~\n",
    "\n",
    "Al crear el modelo en la primer línea hay una opción muy interesante que permite elegir el método paa minimizar $J_R$ , por defecto la opción es \"auto\" y con ella sklearn elige la que considera mejor forma según el tipo de datos. Po rlo general las opciones por defecto funcionan bien, y uno puede dedicarse a probar valores de alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T17:53:14.347475Z",
     "start_time": "2020-09-03T17:53:14.344004Z"
    }
   },
   "source": [
    "## Lasso (Least Absolute Shrinkage and Selection Operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de Regularización Lasso es muy parecido a Ridge, también se adiciona un término siempre positivo a la función de Costo J, pero esta vez la **penalización es $l_1$**, es decir que en vez de sumar los cuadrados de lo $w_j$, sumaremos simplemente sus valores absolutos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Término de Regularización Lasso es entonces:  \n",
    "    \n",
    "$$  \\lambda \\displaystyle\\sum_{j=1}^{n}  | w_j | $$    \n",
    "\n",
    "Al sumarlo a la función de Costo J, la nueva función de Costo queda de la siguiente forma:  \n",
    "\n",
    "$$ J_L = J+ \\lambda \\displaystyle\\sum_{j=1}^{n}  |w_j|$$   \n",
    "\n",
    "es decir:\n",
    "\n",
    "\n",
    "$$ J_L = \\frac{1}{2}  \\displaystyle\\sum_{i=1}^{m} [ h(x_{i}) - y_{i}]²  + \\lambda \\displaystyle\\sum_{j=1}^{n}  |w_j|$$  \n",
    "\n",
    "Pasando a ser esta nueva función de Costo $J_L$ la que deberemos minimizar con respecto a los parámetros $w_j$.  \n",
    "\n",
    "\n",
    "Todas las consideraciones que hemos hecho anteriormente para Ridge son también válidas para Lasso, es decir que un valor de $\\lambda$ pequeño equivale a decir que estamos aplicando poca regularización y los valores de los parámetros serán muy similares a los que obtendríamos en una regresión lineal sin regularización y un valor de $\\lambda$ grande, una fuerte regularización, lo cual indicaría que tenderiamos a disminuir fuertemente los valores de los parámetros.  \n",
    "\n",
    "Por supuesto, habrá que probar con distintos valores de este hiperpárametro hasta encontrar su valor óptimo que será con el que obtengamos por ejemplo el menor RMSE al evaluar el modelo..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de features o variables con Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso trae un **bonus** de regalo. Veamos\n",
    "\n",
    "Como hemos planteado anteriormente al regularizar, tanto con Ridge como con Lasso nuestro objetivo ha sido el de limitar los **grados de libertad** que tiene la Regresión Lineal agregando condiciones a los valores posibles de los parámetros $w_j$.  \n",
    "\n",
    "Estas condiciones obligan a los valores de $w_j$ a asumir valores **más pequeños** que los que se hubieran obtenido con la Regresión Lineal sin regularizar y es en este punto donde Lasso ofrece una característica muy interesante:  \n",
    "\n",
    "> Lasso tiende a transformar en 0 los parámetros de las variables o features de menor relevancia de la hipótesis h(x) del modelo.\n",
    "\n",
    "> **Esto nos permite usar Lasso para eliminar features de escasa relavancia de nuestro Data Set para, inlcusive, aplicar luego cualquier otro modelo**.  \n",
    "\n",
    "Esto es muy importante, porque sabemos que modelos con una gran cantidad de features o variables por tener una alto grado de libertad, tienen tendencia al **overfiting**. \n",
    "\n",
    "Entonces si tenemos demasiadas features podemos pensar en dejar de usar algunas de ellas, pero la pregunta es cuáles. Bueno, Lasso nos brinda una respuesta a esa pregunta. Podemos utilizar Lasso para efectuar **Selección de Variables** de nuestro DataSet original y con el dataset depurado podemos luego aplicar **cualquier otro modelo**, como Árboles de Decisión, kNN, SVM, etc.  \n",
    "\n",
    "Al aplicar Lasso como método de Selección de Variables hay que tener en cuenta un detalle:  \n",
    "\n",
    "- si utlizamos un valor de $\\lambda$ demasiado pequeño, los valores de los parámetros $w_j$ serán los mismos que con Regresión Lineal sin regularizar y quizá niguno se haga 0.  \n",
    "\n",
    "- si, en cambio utilizamos un valor de $\\lambda$ demasiado grande, quizá eliminemos demasiadas variables de nuestro modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso en Scikit- Learn\n",
    "\n",
    "Igual que para Ridge, en  sklearn el hiperparámetro  𝜆  se denomina alpha.\n",
    "\n",
    "\n",
    "La documentación oficial se encuentra en: https://scikitlearn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "El uso básico se vería:\n",
    "~~~\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "modelo_lasso= Lasso(alpha=0.1)\n",
    "modelo_lasso.fit(X_train, y_train)\n",
    "modelo_lasso.score(X_test,y_test)\n",
    "modelo_lasso.predict(X_nuevos)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic - Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más allá del bonus que nos trajo Lasso para eliminar variables de nuestro dataset, pensando ahora en mejorar la performance seguramente se habrá preguntado qué regularización es más eficiente para disminuir el overfiting, la respuesta es que puede ser que la mejor respuesta sea: \"ni una ni otra, sino un poco de cada una de ellas\"; y eso es lo que es Elastic-net.\n",
    "\n",
    "Elastic - Net es simplemente una manera de utilizar ambas posibilidades de regularización, un poco de Ridge y otro poco de Lasso, permitiéndonos elegir cuánta de cada tipo deseamos. El \"truco\" se basa en lo siguiente:  \n",
    "\n",
    "En un caso como éste:  \n",
    "\n",
    "Lasso + Ridge \n",
    "\n",
    "estaríamos sumando \"todo\" Lasso  y \"todo\" Ridge , ahora fíjes en lo siguiente:  \n",
    "\n",
    "r Lasso  + (1 - r) Ridge\n",
    "\n",
    "siendo r un valor entre 0 y 1, entonces fíjese que si decimos \" queremos usar un 70% de Lasso\" ajustaríamos r=0,7 y nos quedaría:  \n",
    "\n",
    "0,7 Lasso  + (1-0,7) Ridge\n",
    "\n",
    "es decir:  \n",
    "\n",
    "0,7 Lasso  + 0,3 Ridge\n",
    "\n",
    "Ahora estaríamo sumando un 70% de Lasso  y un 30% de Ridge .\n",
    "\n",
    "Esto es justamente lo que se hace a la función de Costo original de OLS, J; le sumaremos de esta manera los términos de regularización de Lasso  y Ridge.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de Costo de ElasticNet, $J_E$ será entonces:  \n",
    "\n",
    "$$ J_E = \\frac{1}{2}  \\displaystyle\\sum_{i=1}^{m} [ h(x_{i}) - y_{i}]²  + r \\lambda \\displaystyle\\sum_{j=1}^{n}  |w_j| + (1-r) \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 $$  \n",
    "\n",
    "Como siempre, $J_E$ es una función de los parámetros $w_j$, y se la debe minimizar con respecto a ellos para obtener los valores óptimos de los $w_j$ por ejemplo usando GD o sus derivados.\n",
    "\n",
    "\n",
    "Analicemos un poco el comportamiento del nuevo **hiperparámetro r**:  \n",
    "\n",
    "- Si r=0, desaparecerá el término de Lasso y el modelo tendría sólo regularización Ridge. \n",
    "- Si r=1, desaparecerá el termino de Ridge y la regularización será sólo Lasso.\n",
    "\n",
    "\n",
    "Los hiperparámetros r y $\\lambda$ son configurables en Scikit - learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T19:29:22.750437Z",
     "start_time": "2020-09-03T19:29:22.747477Z"
    }
   },
   "source": [
    "### Elastic Net en Scikit- Learn\n",
    "\n",
    "\n",
    "La Documentación oficial se encuentra en: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "\n",
    "- El hiperparámetro 𝜆 se denomina **alpha**.\n",
    "- El hiperparámetro r se denomina **l1_ratio**\n",
    "\n",
    "\n",
    "El uso básico se vería:\n",
    "\n",
    "~~~\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "modelo_elastic= ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "modelo_elastic.fit(X_train, y_train)\n",
    "modelo_elastic.score(X_test,y_test)\n",
    "modelo_elastic.predict(X_nuevos)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística con Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Regularización también puede aplicarse a problemas de Clasificación, por ejemplo al modelo de Regresión Logística con la intención de disminuir el overfiting que pudiera tener nuestro modelo.  \n",
    "\n",
    "Conceptualmente la idea es exactamente la misma que la explicada anteriormente para la Regresión Lineal, así que simplemente recordemos cómo quedarían las ecuaciones.\n",
    "\n",
    "\n",
    "\n",
    "La función hipótesis para Regresión Logística considerando n features o variables era:  \n",
    "\n",
    "$$ h(x)= \\frac{1}{(1+e^{-(w_0+w_1 x_1 + w_2 x_2 + ...+ w_n x_n)})} $$    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de Costo J para Regresión Logística considerando m observaciones era:  \n",
    "\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^{i=m} [ -y_i log(h(x_i)) - (1-y_i) log(1-h(x_i)) ] $$  \n",
    "\n",
    "que a la hora de minimizar era una función de los parámetros $w_j$ del la hipótesis h(x), también podemos obviar a m o no en la expresión anterior ya que el resultado será el mismo. Luego la quitaremos para que queda más corta la expresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logistica con regularización Ridge:\n",
    "\n",
    "El Término de Regularización de Ridge es el mismo que antes:  \n",
    "\n",
    "$$ \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2$$ \n",
    "\n",
    "Así que simplemente lo sumamos a J\n",
    "\n",
    "\n",
    "$$ J_R = \\sum_{i=1}^{i=m} [ -y_i log(h(x_i)) - (1-y_i) log(1-h(x_i)) ] + \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 $$ \n",
    "\n",
    "No olvide que **no regularizamos el $w_0$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logistica con regularización Lasso\n",
    "\n",
    "$$ J_L = \\sum_{i=1}^{i=m} [ -y_i log(h(x_i)) - (1-y_i) log(1-h(x_i)) ] + \\lambda \\displaystyle\\sum_{j=1}^{n}  |w_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística con regularización Elastic-Net:\n",
    "\n",
    "$$ J_E = \\sum_{i=1}^{i=m} [ -y_i log(h(x_i)) - (1-y_i) log(1-h(x_i)) ] + r \\lambda \\displaystyle\\sum_{j=1}^{n}  |w_j| + (1-r) \\frac{\\lambda}{2} \\displaystyle\\sum_{j=1}^{n}  w_j^2 $$\n",
    "\n",
    "Las consideraciones que hicimos para el valor de r son las mismas que antes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística Regularizada en Scikit - Learn:\n",
    "\n",
    "Documentación oficial de Scikit - Learn:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "**IMPORTANTE** \n",
    "En la implementación de Scikit-Learn no utiliza $\\lambda$ = alpha, sino que utiliza justamente la inversa de $\\lambda$ denominada **C**\n",
    "\n",
    "Es decir que el parámetro $ C = \\frac{1}{\\lambda}$  y por lo tanto:  \n",
    "\n",
    "- **un valor de C pequeño corresponde a una fuerte regularización**  \n",
    "\n",
    "- **un valor de C grande corresponde a una regularización pequeña**.\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contenidos",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "201.225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
